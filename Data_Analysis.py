# -*- coding: utf-8 -*-
"""Data Analysis & Network Science

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1A7UVsfqTRboWP5GtthzEbCA_aAGYMMof
"""

!pip install matplotlib

!pip install vaderSentiment

!pip install transformers

# Import libraries

import nltk
import pandas as pd
import re
from nltk.corpus import stopwords
from sklearn.feature_extraction.text import TfidfVectorizer
from nltk import word_tokenize, pos_tag
from collections import Counter
import networkx as nx
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.feature_extraction.text import TfidfVectorizer
import numpy as np
from transformers import BertTokenizer, BertModel
import torch
import matplotlib.pyplot as plt
from nltk.sentiment.vader import SentimentIntensityAnalyzer
from nltk.corpus import sentiwordnet

nltk.download('averaged_perceptron_tagger')
nltk.download('punkt')
nltk.download('stopwords')

nltk.download('vader_lexicon')
nltk.download('sentiwordnet')
nltk.download('wordnet')

"""**1. Perform pre-processing (use nltk library)**

a. Remove stop-words

b. Remove any hashtag and user account

c. Remove noise, such that, noise is any word that is less than three letters
"""

df = pd.read_csv('/content/hblast_input_data.csv')

stop_words = set(stopwords.words('english'))

def preprocess_tweet(tweet):

    # Remove URLs
    tweet = re.sub(r"http\S+", "", tweet)
    # Remove hashtags and mentions
    tweet = re.sub(r"(#|@)\w+", "", tweet)
    # Remove special characters and numbers
    tweet = re.sub(r"[^a-zA-Z\s]", "", tweet)
    # Remove extra whitespace
    tweet = " ".join(tweet.split())
    # Remove stop words and noise
    words = tweet.split(" ")
    filtered_words = [word for word in words if word.lower() not in stop_words and len(word) >= 3]
    tweet = " ".join(filtered_words)

    return tweet

df['preprocessed_text'] = df['text'].apply(preprocess_tweet)

df.to_csv('preprocessed_tweets.csv', index=False)

"""**2. Text Processing**

a. Find the top 50 keywords from the dataset by tf-idf

b. Keep only the nouns from tweet text and then, find the top 50 nouns from the
dataset by tf-idf




"""

df = pd.read_csv('preprocessed_tweets.csv')

# Function to get nouns from text
def get_nouns(text):
    words = word_tokenize(text)
    tagged_words = pos_tag(words)
    nouns = [word for word, pos in tagged_words if pos.startswith('N')]
    return ' '.join(nouns)

df['nouns'] = df['preprocessed_text'].apply(get_nouns)

# Top 50 keywords
tfidf_vectorizer = TfidfVectorizer(max_features=50)
tfidf_keywords = tfidf_vectorizer.fit_transform(df['preprocessed_text'])
keywords = tfidf_vectorizer.get_feature_names_out()

# Top 50 nouns
tfidf_vectorizer_nouns = TfidfVectorizer(max_features=50)
tfidf_nouns = tfidf_vectorizer_nouns.fit_transform(df['nouns'])
top_nouns = tfidf_vectorizer_nouns.get_feature_names_out()

# Print Top 50 keywords
print("Top 50 Keywords:")
for i, keyword in enumerate(keywords):
    print(f"{i + 1}. {keyword}")

# Print Top 50 nouns
print("\nTop 50 Nouns:")
for i, noun in enumerate(top_nouns):
    print(f"{i + 1}. {noun}")

"""**3. Network Science and Pretrained Language Model**

a. Create a graph, G where nodes are tweets and the edges represent cosine
similarity between the tweet text.

Basically, two tweets are connected by an edge
if their cosine similarity is greater than 0.5 else there is no edge.

For graph
creation, use networkx and for tweets, use the tweet text after performing all the
steps of (1) pre-processing

"""

df = pd.read_csv('preprocessed_tweets.csv')

# Calculate TF-IDF vectors
tfidf_vectorizer = TfidfVectorizer()
tfidf_matrix = tfidf_vectorizer.fit_transform(df['preprocessed_text'])

# Calculate cosine similarity between all tweet pairs
cosine_similarities = cosine_similarity(tfidf_matrix, tfidf_matrix)

G = nx.Graph()

# Add nodes for tweets
for i, tweet in enumerate(df['preprocessed_text']):
    G.add_node(i, tweet=tweet)

# Add edges for tweets with cosine similarity greater than 0.5
num_tweets = len(df)
for i in range(num_tweets):
    for j in range(i + 1, num_tweets):
        if cosine_similarities[i][j] > 0.5:
            G.add_edge(i, j)

degree_distribution = dict(nx.degree(G))
degrees = list(degree_distribution.values())

# Histogram
plt.hist(degrees, bins=30, alpha=0.5, color='b', edgecolor='black')
plt.xlabel('Degree (Number of Connections)')
plt.ylabel('Number of Nodes (Tweets)')
plt.title('Degree Distribution Histogram')
plt.show()

"""**b. From this graph,**

i. Calculate the degree distribution of the graph

ii. Calculate degree centrality of all tweets and print the top 10 tweets

iii. Calculate page rank centrality of all the tweets and print the top 10 tweets

iv. Share your comments on which centrality is better in this scenario with
examples.

***We calculate Degree centrality and Page rank centrality.
As we can see in the output, the former gives us multiple duplicate tweets as it has been retweeted multiple times. However, it might not be well-connected to other critical tweets.
The latter, on the other hand, provides more links to  multiple influential tweets. This would help find more well-connected tweets, and thus would be a better centrality measure.
At the same, we see that the scores for Degree centrality measure are higher than for PageRank. Despite the high scores, PageRank centrality would be more reliable as a measure in this scenario given that it focuses more on the quality of connections.***



v. Calculate the clustering co-efficient of the whole graph, top 10 tweets and
bottom 10 tweets. You can consider degree centrality for understanding
the top 10 tweets
"""

# i. Calculate the degree distribution of the graph
degree_distribution = dict(G.degree())

# ii. Calculate degree centrality of all tweets and print the top 10 tweets
degree_centrality = nx.degree_centrality(G)
top_10_degree_centrality = sorted(degree_centrality.items(), key=lambda x: x[1], reverse=True)[:10]

print("Top 10 Tweets by Degree Centrality:")
for tweet_id, centrality in top_10_degree_centrality:
    tweet_text = G.nodes[tweet_id]['tweet']
    print(f"Tweet ID: {tweet_id}, Centrality: {centrality}, Tweet Text: {tweet_text}")

# iii. Calculate PageRank centrality of all the tweets and print the top 10 tweets
pagerank_centrality = nx.pagerank(G)
top_10_pagerank_centrality = sorted(pagerank_centrality.items(), key=lambda x: x[1], reverse=True)[:10]

print("\nTop 10 Tweets by PageRank Centrality:")
for tweet_id, centrality in top_10_pagerank_centrality:
    tweet_text = G.nodes[tweet_id]['tweet']
    print(f"Tweet ID: {tweet_id}, Centrality: {centrality}, Tweet Text: {tweet_text}")

# v. Calculate the clustering coefficient
clustering_coefficient_whole = nx.average_clustering(G)
print(f"\nClustering Coefficient of the Whole Graph: {clustering_coefficient_whole}")

# Top 10 tweets by degree centrality
top_10_tweets_by_degree = [tweet_id for tweet_id, _ in top_10_degree_centrality]
subgraph_top_10 = G.subgraph(top_10_tweets_by_degree)
clustering_coefficient_top_10 = nx.average_clustering(subgraph_top_10)

# Bottom 10 tweets by degree centrality
bottom_10_degree_centrality = sorted(degree_centrality.items(), key=lambda x: x[1])[:10]
bottom_10_tweets_by_degree = [tweet_id for tweet_id, _ in bottom_10_degree_centrality]
subgraph_bottom_10 = G.subgraph(bottom_10_tweets_by_degree)
clustering_coefficient_bottom_10 = nx.average_clustering(subgraph_bottom_10)

print(f"Clustering Coefficient of the Top 10 Tweets: {clustering_coefficient_top_10}")
print(f"Clustering Coefficient of the Bottom 10 Tweets: {clustering_coefficient_bottom_10}")

"""**c. Repeat step (a) with cosine similarity calculated based on pre-trained BERT
uncased embedding and therefore, generate a graph G1 where two tweets are
connected by an edge if their cosine similarity between bert embedding is greater
than 0.5 else there is no edge.**

i. Repeat step b (i to v)

1. Any reasoning which one is better? BERT or text based? Provide
examples for each (i to v)

"""

# Initialize a BERT model and tokenizer
bert_model = BertModel.from_pretrained('bert-base-uncased')
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

# Function to get BERT embeddings for a given text
def get_bert_embedding(text):

    tokens = tokenizer(text, padding=True, truncation=True, return_tensors="pt")

    with torch.no_grad():
        output = bert_model(**tokens)

    cls_embedding = output.last_hidden_state[0][0].numpy()

    return cls_embedding

# Calculate BERT embeddings for all tweets
bert_embeddings = [get_bert_embedding(text) for text in df['preprocessed_text']]

# Calculate cosine similarity between BERT embeddings
cosine_similarities_bert = cosine_similarity(bert_embeddings, bert_embeddings)

G1 = nx.Graph()

for i, tweet in enumerate(df['preprocessed_text']):
    G1.add_node(i, tweet=tweet)

# Add edges for tweets with cosine similarity greater than 0.5
num_tweets = len(df)
for i in range(num_tweets):
    for j in range(i + 1, num_tweets):
        if cosine_similarities_bert[i][j] > 0.5:
            G1.add_edge(i, j)

# i. Calculate the degree distribution of G1
degree_distribution_G1 = dict(G1.degree())
print("Degree Distribution of G1:")
print(degree_distribution_G1)

# ii. Calculate degree centrality of all tweets in G1 and print the top 10 tweets
degree_centrality_G1 = nx.degree_centrality(G1)
top_10_degree_centrality_G1 = sorted(degree_centrality_G1.items(), key=lambda x: x[1], reverse=True)[:10]
print("Top 10 Tweets by Degree Centrality in G1:")
for tweet_id, centrality in top_10_degree_centrality_G1:
    tweet_text = G1.nodes[tweet_id]['tweet']
    print(f"Tweet ID: {tweet_id}, Centrality: {centrality}, Tweet Text: {tweet_text}")

# iii. Calculate PageRank centrality of all the tweets in G1 and print the top 10 tweets
pagerank_centrality_G1 = nx.pagerank(G1)
top_10_pagerank_centrality_G1 = sorted(pagerank_centrality_G1.items(), key=lambda x: x[1], reverse=True)[:10]
print("Top 10 Tweets by PageRank Centrality in G1:")
for tweet_id, centrality in top_10_pagerank_centrality_G1:
    tweet_text = G1.nodes[tweet_id]['tweet']
    print(f"Tweet ID: {tweet_id}, Centrality: {centrality}, Tweet Text: {tweet_text}")

# v. Calculate the clustering coefficient of G1, top 10 tweets, and bottom 10 tweets
clustering_coefficient_whole_G1 = nx.average_clustering(G1)
print(f"Clustering Coefficient of G1: {clustering_coefficient_whole_G1}")

# Top 10 tweets by degree centrality in G1
top_10_tweets_by_degree_G1 = [tweet_id for tweet_id, _ in top_10_degree_centrality_G1]
subgraph_top_10_G1 = G1.subgraph(top_10_tweets_by_degree_G1)
clustering_coefficient_top_10_G1 = nx.average_clustering(subgraph_top_10_G1)
print(f"Clustering Coefficient of the Top 10 Tweets in G1: {clustering_coefficient_top_10_G1}")

# Bottom 10 tweets by degree centrality in G1
bottom_10_degree_centrality_G1 = sorted(degree_centrality_G1.items(), key=lambda x: x[1])[:10]
bottom_10_tweets_by_degree_G1 = [tweet_id for tweet_id, _ in bottom_10_degree_centrality_G1]
subgraph_bottom_10_G1 = G1.subgraph(bottom_10_tweets_by_degree_G1)
clustering_coefficient_bottom_10_G1 = nx.average_clustering(subgraph_bottom_10_G1)
print(f"Clustering Coefficient of the Bottom 10 Tweets in G1: {clustering_coefficient_bottom_10_G1}")

"""#**Comments for the output above:**
In the BERT-based graph, both Degree Centrality and PageRank Centrality seem to have failed to differentiate between tweets giving all the top 10 tweets the same score. These scores could be attributed to the small sample size of the dataset.

However, in comparison to the text-based graph, it has certainly performed better with much higher scores and clustering coefficients. This is because BERT-based embeddings are likely to outperform text-based embeddings in capturing semantic similarity, which in turn yields better results for centrality and clustering coefficient in this scenario.

Therefore, BERT-based embeddings are preferred in this scenario, as they better capture the nuances in tweet similarities.

**4. Sentiment Calculation**

a. Calculate the sentiment of each tweet by Vader sentiment analyzer and SentiNet.

Provide three examples of where Vader fails and 3 examples where SentiNet
fails
"""

def analyze_sentiment(tweet):
    # Initialize VADER sentiment analyzer
    vader_analyzer = SentimentIntensityAnalyzer()

    # Use VADER to get sentiment scores
    vader_scores = vader_analyzer.polarity_scores(tweet)

    # Tokenize the tweet for SentiWordNet analysis
    tokens = nltk.word_tokenize(tweet)

    # Initialize sentiment scores for SentiWordNet
    pos_score = neg_score = obj_score = 0

    # Use SentiWordNet to calculate sentiment scores
    for token in tokens:
        synsets = list(sentiwordnet.senti_synsets(token))
        if synsets:
            synset = synsets[0]
            pos_score += synset.pos_score()
            neg_score += synset.neg_score()
            obj_score += synset.obj_score()

    # Normalize sentiment scores
    sentiment = {
        "VADER_Positive": vader_scores['pos'],
        "VADER_Neutral": vader_scores['neu'],
        "VADER_Negative": vader_scores['neg'],
        "SentiWordNet_Positive": pos_score,
        "SentiWordNet_Negative": neg_score,
        "SentiWordNet_Neutral": obj_score,
    }

    return sentiment

# Load your DataFrame with tweets and preprocessed tweets
# Replace 'df' with your actual DataFrame
df['sentiment_scores'] = df['preprocessed_text'].apply(analyze_sentiment)

# Display the resulting DataFrame with sentiment scores
print(df)
df.to_csv('sentiments_tweets.csv', index=False)

"""#**Examples where VADER and SentiWordNet fail to detect the right sentiment.**

#**SentiWordNet**

**Example 1**  Reports of explosion from busy commercial area in Hyderabad http://t.co/gVwirDH85z.	{'SentiWordNet_Positive': 0.375, 'SentiWordNet_Negative': 0.0, 'SentiWordNet_Neutral': 5.625}


**Example 2** Request to @PMOIndia. Please Monitor tweets spreading rumors or trying to give #Hyderabad blast communal color. Pls block handles ASAP.	{'SentiWordNet_Positive': 0.625, 'SentiWordNet_Negative': 0.125, 'SentiWordNet_Neutral': 13.25}


**Example 3** Seven killed in Hyderabad blast, several injured: Times Now http://t.co/SkQ0I6kZd0.	{'SentiWordNet_Positive': 0.5, 'SentiWordNet_Negative': 1.625, 'SentiWordNet_Neutral': 4.875}


**In the above three examples, SentiWordNet falsely assigns a slight positive score of 0.375, 0.625, and 0.5 respectively to these tweets even though they do not project positive sentiments. Moreover, in the first tweet it does not capture the negative sentiment at all (0.0), while capturing the negative very little in the second tweet (0.125).**
"""



"""#**VADER**

**Example 1**
Damn, Hyderabad blasts, stay safe Hyderabad :(.	Damn Hyderabad blasts stay safe Hyderabad	{'VADER_Positive': 0.302, 'VADER_Neutral': 0.417, 'VADER_Negative': 0.281, }


***Falsely captures a positive sentiment.***

**Example 2**
My condolences go out to the families affected by the#Hyderabad #blast such an unfortunate event :-(.	condolences families affected unfortunate event	{'VADER_Positive': 0.0, 'VADER_Neutral': 0.395, 'VADER_Negative': 0.605,}

***Fails to capture the positive sentiment at all while giving a higher negative score.***

**Example 3**

TerroristAttack in Hyderabad! "At Least 11 Killed in Hyderabad Blasts" http://t.co/xOKYQn651r via @WSJ #HyderabadBlasts.	Hyderabad Least Killed Hyderabad Blasts via	{'VADER_Positive': 0.418, 'VADER_Neutral': 0.582, 'VADER_Negative': 0.0, }

***Fails to capture any negative sentiment at all even though the tweet is about an extremely unfortunate event.***
"""